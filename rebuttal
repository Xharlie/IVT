View Reviews
Paper ID3123
Paper TitleGrid-GCN for Fast and Scalable Point Cloud Learning
Reviewer #1
Questions
1. I am confused about the storage method of the voxel-point map. Only non-empty voxels are stored? Or all voxels in the space are stored in a matrix (X, Y, Z, n_v)?
If all voxels are stored, then it seems that most voxels are empty. How's the memory cost?
If only non-empty voxels are stored, then how to find the neighboring voxels efficiently?

The voxel-point mapping is implemented with an ``occupied array`` (M, n_v) storing the indices of the points in the M occupied voxels (the cluster number M is usually big enough), and a matrix (X, Y, Z) that stores the indices of the current voxel in ``occupied array``. Therefore the memory cost is (X * Y * Z + M * n_v) ints, usually less than 5m each scene. And points in the neighboring voxel can be extracted with direct array accesses between these two data structures. 


2. Some voxel-based methods (e.g., SparseConvNet, MinkowskiNet) achieve sota performance on 3D segmentation tasks and also seem to be efficient on large-scale scenes. Could you give a comparison of inference time (to get a per-point semantic label of the whole scene) with those methods?

SparseConvNet and MinkowskiNet do not report the latency in their paper, but according to MinkowskiNet's github example, it has a latency of 103ms on Scannet, with batch size 1 and 81920 points per scene. Our model has a latency of 19ms (line 832) under the same condition. Please note that both Grid-GCN and MinkUNet34C has an initial voxel size of 0.05. Due to the new PAMI-TC policy we cannot show the result here but we will add comparisons in our final version. The huge speed difference might be attributed to implementation and hyper parameter selection. Comparatively, sparse convolution allows very deep neural network architecture to be applied. However, our model keeps the location granularity of each point and more flexible since we can adjust K, M, N and voxel size for each layer, therefore has a better control of the trade-off between speed and accuracy. 


3. What is the voxel size in your experiments? Would the voxel size affect the performance and runtime?

The voxel sizes are 0.05, 0.15 and 0.45 for 3 layers in all experiments. 0.05 is selected as the volume of its kernel cube ((0.05*3)^3) is roughly equals to the volume of the ball in PointNet++. We didn't show the analysis in the submission which involves too much details even we do explore the trade-off. We will report it in the final version (cannot post it here due to the PAMI-TC policy). Under the same K,M and N, the smaller the voxel size the more voxels we need to process but better concurrency the gpu can enjoy (fewer collisions between different threads). Smaller voxel size also preserves more local features but shorter range during deeper aggragation. We will show that our choices of voxel sizes are reasonable in the analysis. 

4. Is the grid context pooling effective in the segmentation task?

However less effective than in classification, the grid context pooling improves the OA by 0.1 for Scannet and 0.05 for S3DIS. Since segmentation is relatively harder, more local information is harder to learn by our very simple back bone network (only 3 downsample layers).

5. In table 6, there is no ablation on pooling and weight with K and channels unchanged.

Between the first and second rows, we report the ablation on weight but not on pooling with K and Channels unchanged. We will add more ablations of pooling and weight in the final version.(Cannot report them here due to PAMI-TC policy)




Reviewer #2
Questions

However, the novelty of the propose data structuring strategy is very limited. This kind of data structures have been extensively used in the past to speed up point cloud queries. “Growing Neural Gas Efficiently” is just an example of this kind of data structures applied to point indexing. In fact, looking back, this idea has been also extensively exploited in graphics by using Octrees and other similar data structures, in fact one improved version of the proposed uniform sampling grid, is to use an adaptive grid, where cells in the grid are adaptively sized based on the point distribution. In 3D also known as Octrees.
The ablation study is very limited, even though the authors claimed many advantages about the proposed architecture, only final results are shown, so it´s not clear, how each of these different contributions, such as coverage weight, grid context pooling or the sampling strategy itself, affect the final results.
Some parts of the network are based on greedy approaches or heuristics, I would recommend authors to extend the presented work by addressing learning-based approach for those stages. Random voxel sampling or the definition of the K points for each point group.
Overall the main contribution of this work is the application of an efficient data structure on top of existing GCN-based networks, therefore I find it very limited in terms of novelty.
 
Novelty in data structuring and differences with mentioned methods: 
The design of Grid-GCN is to fully preserve the location granularity in computation as point-based model, and leverage efficiency of voxel space in data structuring. Our method records the non-empty voxel without building any hierachy like Octree, since the purposes of voxelization are to answer: 1. which part of the space is non-empty, 2.what are the neighbors of these non-empty voxels. If these two goals can be achieved by a point cloud scan, adaptive grid methods (e.g.octrees or kd-trees) are not neccessary. However if a model in computation quantizes the point to voxel cells and directly apply convolution on them, these occtree and kd-tree are indeed helpful, which is not in our case. In Table 3 and Table 4, we show that our model outperforms octree or kd-tree based methods by a large margin.

In line 195-201, we also discuss the self-organizing map methods (a category includes Growing Neural Gas), Gumble Subset Sampling and tree-based methods. Since they need preprocessing or un-supervised learning to fit the distribution, no matter how fast the methods are, it is hard to fulfill the need of today's large-scale real-time point cloud applications. Our method although less complicated but is the first one to consider occupied space coverage and the only one that can achieve a latency of 19.8ms on 81920 points per scene among point cloud learning methods. 


Novelty in graph convolution:
Our contributions in graph convolution are two-fold and are visualized in Figure 4: 1. we utilize coverage weight as a useful geometric feature, 2. we introduce context pooling to get semantic context features which induce almost no extra cost under our framework (line 453-458). 


Ablation studies:
In table 6, we provide ablation studies on coverage weight, grid context pooling, network channel number and neighbor number K. We will 



Reviewer #3
Questions

1). Should RVS and CAS both be used in sampling in sequence? Why are they separated in experiments?

RVS and CAS are two different strategy choices that are not used in sequence. RVS selects center voxel randomly, in our cuda implementation, we have a concurrent randomization mechanism. CAS selects first M occupied voxels as center voxels, then uses the challenger-incumbent method (line 328 to 333) to replace the selected voxels by better candidates.


2). What is the connection between Cube query and KNN? It seemed that you offer two separate group methods but donot give which is better or which one should be chosen under certain circumstances.

We provide both Cube Query and KNN to match two strategies (ball query and KNN) that most point cloud models use (pointnet++, PointCNN, etc.). Therefore, our data structuring module can support other powerful models. Paired with our neural network, we find using Cube query in all layers provides better performance but KNN might be a better choice if paired with other networks. We will mention this in our final version and provide ablation studies for grouping choices.    


3). Content point is a very important concept in this paper, the selection strategy for it is described as: the points contained in voxel neighbors. Do voxel neighbors refer to the 24 points around the center voxel or the eight points bordering the center voxel? Recommend to specify the selection strategy of voxel neighbors.

To clarify the concept of Context points, we visualize the selection in Figure 2(d). Line 233 to 235 explain that the context point of center voxel(2,1) are the yellow points inside its 3X3 neighborhood, which means all 21 yellow points inside the lower red box. We thank the reviewer of pointing out the confusion and will improve our explanation in the final version. 



4). Since grid content pooling extracts features from content points instead of K node points, I noticed that K node points obtained by grouping are only used to calculate barycenter, so why choose barycenter as the group center? Section3.3 says the premise of grid content pooling is that the group center is the barycenter of K node points，but lacks an explanation for this. In addition, the calculation process of barycenter also needs to be more clear.

In 3.3 especially Figure 4 (d), we show that K node points are used as graph node of graph convolution, and context features extracted from context points are used as features on the virtual center node(line 429). There are two strategies to select group center: 1. (line 420-423) pick one of the graph nodes as center node, use the feature on the center node to calculate edge relation(e.g. GACNet) 2. create a virutal center node on the barycenter of the graph nodes, therefore the edge relation can only be calculated by gemotric distances but not any semantic features (e.g. RS-Conv). To fill the gap, (line 429-431) here we use Context features as the semantic features on the virtual center node to calculate edge relation. We will rephrase the explanation to clarify the confusion in our final version.

The calculation of barycenter is explained in appendix E.




5). Since the paper mentioned that a new calculation method of edge attention function is proposed, a clear formula should be derived instead of simply expressing it as e = f (influencing factor).

We derive detailed edge attention function as equation 6 (line 462)





6). The proposed RVS and CAS are expressed too brief. It is better to be more elaborate.

Our description on RVS and CAS are in line 315 to line 350, and line 031 to 044 in appendix A. We will further elaborate them by adding algorithm blocks (similar to appendix D) for RVS and CAS in our final version.




7). Section 5.1 compares Grid-GCN with more than ten methods on ModelNet dataset, but excludes the newest methods such as 3D2SeqViews and SeqViews2SeqLabels which seems to perform better on Leaderboard of ModelNet. This makes the superiority of your method unconvincing.

Since our paper focuses on high speed large-scale point cloud learning, we only compare with methods with point cloud input (kd-tree and octree are also built from point cloud input). However to make our result more convincing, we will cite 3D2SeqViews and SeqViews2SeqLabels and include them, even they take image view as input.


8). Does the size of voxel have an effect on the performance of Grid GCN? Suggest to join some experiments to discuss this issue.

similar to R1Q3
